{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertTokenizer\n",
    "import json\n",
    "import pandasql as ps\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209527\n"
     ]
    }
   ],
   "source": [
    "file_data = r\"D:\\Repo\\Other_Data\\News_Category_Dataset_v3.json\"\n",
    "with open(file_data, \"r\", encoding=\"utf-8\") as file:\n",
    "    data = [json.loads(line) for line in file]\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "link : https://www.huffpost.com/entry/covid-boosters-uptake-us_n_632d719ee4b087fae6feaac9\n",
      "headline : Over 4 Million Americans Roll Up Sleeves For Omicron-Targeted COVID Boosters\n",
      "category : U.S. NEWS\n",
      "short_description : Health experts said it is too early to predict whether demand would match up with the 171 million doses of the new boosters the U.S. ordered for the fall.\n",
      "authors : Carla K. Johnson, AP\n",
      "date : 2022-09-23\n"
     ]
    }
   ],
   "source": [
    "for i in data[0]:\n",
    "    print(i, \":\", data[0][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            headline   category  \\\n",
      "0  Over 4 Million Americans Roll Up Sleeves For O...  U.S. NEWS   \n",
      "1  American Airlines Flyer Charged, Banned For Li...  U.S. NEWS   \n",
      "2  23 Of The Funniest Tweets About Cats And Dogs ...     COMEDY   \n",
      "3  The Funniest Tweets From Parents This Week (Se...  PARENTING   \n",
      "4  Woman Who Called Cops On Black Bird-Watcher Lo...  U.S. NEWS   \n",
      "\n",
      "                                   short_description               authors  \n",
      "0  Health experts said it is too early to predict...  Carla K. Johnson, AP  \n",
      "1  He was subdued by passengers and crew when he ...        Mary Papenfuss  \n",
      "2  \"Until you have a dog you don't understand wha...         Elyse Wanshel  \n",
      "3  \"Accidentally put grown-up toothpaste on my to...      Caroline Bologna  \n",
      "4  Amy Cooper accused investment firm Franklin Te...        Nina Golgowski  \n"
     ]
    }
   ],
   "source": [
    "# Chuyển dữ liệu JSON thành DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "df = df.drop(columns=[\"link\", \"date\"])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         category  count_category\n",
      "0        POLITICS           35602\n",
      "1        WELLNESS           17945\n",
      "2   ENTERTAINMENT           17362\n",
      "3          TRAVEL            9900\n",
      "4  STYLE & BEAUTY            9814\n",
      "5       PARENTING            8791\n",
      "6  HEALTHY LIVING            6694\n",
      "7    QUEER VOICES            6347\n",
      "8    FOOD & DRINK            6340\n",
      "9        BUSINESS            5992\n"
     ]
    }
   ],
   "source": [
    "# Đếm category\n",
    "query = \"SELECT category, COUNT(category) AS count_category FROM df GROUP BY category ORDER BY count_category DESC LIMIT 10\"\n",
    "result = ps.sqldf(query, locals())\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Over 4 Million Americans Roll Up Sleeves For Omicron-Targeted COVID Boosters\n",
      "American Airlines Flyer Charged, Banned For Life After Punching Flight Attendant On Video\n"
     ]
    }
   ],
   "source": [
    "# Khai phá dữ liệu\n",
    "print(df['headline'].iloc[0])\n",
    "print(df['headline'].iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\duclh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Tiền xử lý dữ liệu bằng cách gộp headline, short_description, authors thành một chuỗi đầu vào\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "def preprocess_text(row):\n",
    "    text = row[\"headline\"] + \" \" + row[\"short_description\"] + \" \" + row[\"authors\"]\n",
    "    return text.lower().strip()\n",
    "\n",
    "df[\"text\"] = df.apply(preprocess_text, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels (category)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"category\"] = label_encoder.fit_transform(df[\"category\"])\n",
    "num_classes = len(label_encoder.classes_)  # Số lượng nhãn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize dữ liệu văn bản\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "MAX_LEN = 50  # Giới hạn độ dài câu\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = tokenizer(text, padding=\"max_length\", truncation=True, max_length=MAX_LEN, return_tensors=\"pt\")\n",
    "    return tokens[\"input_ids\"].squeeze(0), tokens[\"attention_mask\"].squeeze(0)\n",
    "\n",
    "df[\"tokens\"] = df[\"text\"].apply(lambda x: tokenize_text(x)[0])\n",
    "df[\"attention_masks\"] = df[\"text\"].apply(lambda x: tokenize_text(x)[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([167621, 50]), X_test: torch.Size([41906, 50])\n",
      "y_train: torch.Size([167621]), y_test: torch.Size([41906])\n",
      "masks_train: torch.Size([167621, 50]), masks_test: torch.Size([41906, 50])\n"
     ]
    }
   ],
   "source": [
    "# Chia dữ liệu thành tập huấn luyện và tập kiểm tra\n",
    "\n",
    "X = torch.stack(df[\"tokens\"].tolist())  # Dữ liệu đầu vào\n",
    "y = torch.tensor(df[\"category\"].values, dtype=torch.long)  # Nhãn\n",
    "masks = torch.stack(df[\"attention_masks\"].tolist())  # Attention masks\n",
    "\n",
    "# Chuyển tensor về numpy trước khi chia\n",
    "X_list = X.detach().cpu().tolist()\n",
    "y_list = y.detach().cpu().tolist()\n",
    "masks_list = masks.detach().cpu().tolist()\n",
    "\n",
    "# Chia tập dữ liệu (dùng random_state để tái lập kết quả)\n",
    "X_train_list, X_test_list, y_train_list, y_test_list, masks_train_list, masks_test_list = train_test_split(\n",
    "    X_list, y_list, masks_list, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Chuyển lại về tensor\n",
    "X_train, X_test = torch.tensor(X_train_list), torch.tensor(X_test_list)\n",
    "y_train, y_test = torch.tensor(y_train_list), torch.tensor(y_test_list)\n",
    "masks_train, masks_test = torch.tensor(masks_train_list), torch.tensor(masks_test_list)\n",
    "\n",
    "\n",
    "# In kích thước để kiểm tra\n",
    "print(f\"X_train: {X_train.shape}, X_test: {X_test.shape}\")\n",
    "print(f\"y_train: {y_train.shape}, y_test: {y_test.shape}\")\n",
    "print(f\"masks_train: {masks_train.shape}, masks_test: {masks_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Khởi tạo mô hình RNN/LSTM\n",
    "\n",
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "EMBED_DIM = 128\n",
    "HIDDEN_DIM = 256\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.5\n",
    "LEARNING_RATE = 1e-3\n",
    "EPOCHS = 1\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "\n",
    "embedding = nn.Embedding(VOCAB_SIZE, EMBED_DIM).to(device)\n",
    "lstm = nn.LSTM(EMBED_DIM, HIDDEN_DIM, num_layers=NUM_LAYERS, batch_first=True, dropout=DROPOUT).to(device)\n",
    "fc = nn.Linear(HIDDEN_DIM, num_classes).to(device)\n",
    "dropout = nn.Dropout(DROPOUT).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(list(embedding.parameters()) + list(lstm.parameters()) + list(fc.parameters()), lr=LEARNING_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chuyển dữ liệu thành DataLoader\n",
    "\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Loss: 2.2922\n"
     ]
    }
   ],
   "source": [
    "# Vòng lặp huấn luyện\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        inputs, targets = batch\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        embedded = embedding(inputs)\n",
    "        lstm_out, _ = lstm(embedded)\n",
    "        final_hidden_state = lstm_out[:, -1, :]\n",
    "        output = fc(dropout(final_hidden_state))\n",
    "        \n",
    "        loss = criterion(output, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {total_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5532\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "          ARTS       0.49      0.20      0.29       293\n",
      "ARTS & CULTURE       0.86      0.45      0.59       275\n",
      "  BLACK VOICES       0.58      0.25      0.35       889\n",
      "      BUSINESS       0.24      0.18      0.21      1216\n",
      "       COLLEGE       0.66      0.14      0.24       202\n",
      "        COMEDY       0.64      0.42      0.51      1022\n",
      "         CRIME       0.27      0.39      0.32       713\n",
      "CULTURE & ARTS       0.00      0.00      0.00       202\n",
      "       DIVORCE       0.84      0.56      0.67       664\n",
      "     EDUCATION       0.00      0.00      0.00       209\n",
      " ENTERTAINMENT       0.51      0.78      0.62      3419\n",
      "   ENVIRONMENT       0.11      0.00      0.01       313\n",
      "         FIFTY       0.29      0.08      0.12       263\n",
      "  FOOD & DRINK       0.64      0.68      0.66      1270\n",
      "     GOOD NEWS       0.75      0.33      0.46       270\n",
      "         GREEN       0.25      0.01      0.01       532\n",
      "HEALTHY LIVING       0.49      0.48      0.48      1302\n",
      " HOME & LIVING       0.85      0.50      0.63       879\n",
      "        IMPACT       0.49      0.03      0.06       673\n",
      " LATINO VOICES       0.84      0.42      0.56       238\n",
      "         MEDIA       0.84      0.23      0.36       607\n",
      "         MONEY       0.29      0.01      0.01       355\n",
      "     PARENTING       0.38      0.54      0.45      1768\n",
      "       PARENTS       0.68      0.41      0.51       795\n",
      "      POLITICS       0.68      0.87      0.76      7155\n",
      "  QUEER VOICES       0.47      0.62      0.54      1262\n",
      "      RELIGION       0.56      0.25      0.34       530\n",
      "       SCIENCE       0.51      0.07      0.12       424\n",
      "        SPORTS       0.76      0.33      0.46      1014\n",
      "         STYLE       0.74      0.49      0.59       464\n",
      "STYLE & BEAUTY       0.71      0.77      0.74      1975\n",
      "         TASTE       0.65      0.21      0.32       427\n",
      "          TECH       0.53      0.21      0.30       398\n",
      " THE WORLDPOST       0.32      0.49      0.39       741\n",
      "        TRAVEL       0.49      0.66      0.56      2021\n",
      "     U.S. NEWS       0.04      0.00      0.01       269\n",
      "      WEDDINGS       0.69      0.76      0.72       709\n",
      "    WEIRD NEWS       0.34      0.40      0.37       550\n",
      "      WELLNESS       0.54      0.78      0.64      3672\n",
      "         WOMEN       0.56      0.21      0.31       727\n",
      "    WORLD NEWS       0.28      0.06      0.10       665\n",
      "     WORLDPOST       0.29      0.46      0.36       534\n",
      "\n",
      "      accuracy                           0.55     41906\n",
      "     macro avg       0.50      0.35      0.37     41906\n",
      "  weighted avg       0.55      0.55      0.52     41906\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\duclh\\AppData\\Local\\anaconda3\\envs\\ml_venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\duclh\\AppData\\Local\\anaconda3\\envs\\ml_venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\duclh\\AppData\\Local\\anaconda3\\envs\\ml_venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Đánh giá mô hình\n",
    "\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        inputs, targets = batch\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        embedded = embedding(inputs)\n",
    "        lstm_out, _ = lstm(embedded)\n",
    "        final_hidden_state = lstm_out[:, -1, :]\n",
    "        output = fc(dropout(final_hidden_state))\n",
    "        \n",
    "        _, predicted = torch.max(output, 1)\n",
    "        \n",
    "        y_pred.extend(predicted.cpu().tolist())\n",
    "        y_true.extend(targets.cpu().tolist())\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(classification_report(y_true, y_pred, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Category: WORLDPOST\n"
     ]
    }
   ],
   "source": [
    "# Dự đoán với dữ liệu mới\n",
    "\n",
    "def predict_category(text):\n",
    "    tokenized_text, _ = tokenize_text(text)\n",
    "    tokenized_text = tokenized_text.unsqueeze(0).to(device)  # Thêm batch dimension\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embedded = embedding(tokenized_text)\n",
    "        lstm_out, _ = lstm(embedded)\n",
    "        final_hidden_state = lstm_out[:, -1, :]\n",
    "        output = fc(dropout(final_hidden_state))\n",
    "        _, predicted = torch.max(output, 1)\n",
    "\n",
    "    return label_encoder.inverse_transform([predicted.cpu().item()])[0]\n",
    "\n",
    "new_text = \"Apple announces new iPhone with AI-powered camera\"\n",
    "print(\"Predicted Category:\", predict_category(new_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lưu mô hình\n",
    "\n",
    "torch.save({\n",
    "    \"embedding\": embedding.state_dict(),\n",
    "    \"lstm\": lstm.state_dict(),\n",
    "    \"fc\": fc.state_dict()\n",
    "}, \"text_rnn_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
