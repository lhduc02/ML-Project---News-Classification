{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertTokenizer\n",
    "import json\n",
    "import pandasql as ps\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209527\n"
     ]
    }
   ],
   "source": [
    "file_data = \"../data/News_Category_Dataset_v3.json\"\n",
    "with open(file_data, \"r\", encoding=\"utf-8\") as file:\n",
    "    data = [json.loads(line) for line in file]\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "link : https://www.huffpost.com/entry/covid-boosters-uptake-us_n_632d719ee4b087fae6feaac9\n",
      "headline : Over 4 Million Americans Roll Up Sleeves For Omicron-Targeted COVID Boosters\n",
      "category : U.S. NEWS\n",
      "short_description : Health experts said it is too early to predict whether demand would match up with the 171 million doses of the new boosters the U.S. ordered for the fall.\n",
      "authors : Carla K. Johnson, AP\n",
      "date : 2022-09-23\n"
     ]
    }
   ],
   "source": [
    "for i in data[0]:\n",
    "    print(i, \":\", data[0][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            headline   category  \\\n",
      "0  Over 4 Million Americans Roll Up Sleeves For O...  U.S. NEWS   \n",
      "1  American Airlines Flyer Charged, Banned For Li...  U.S. NEWS   \n",
      "2  23 Of The Funniest Tweets About Cats And Dogs ...     COMEDY   \n",
      "3  The Funniest Tweets From Parents This Week (Se...  PARENTING   \n",
      "4  Woman Who Called Cops On Black Bird-Watcher Lo...  U.S. NEWS   \n",
      "\n",
      "                                   short_description               authors  \n",
      "0  Health experts said it is too early to predict...  Carla K. Johnson, AP  \n",
      "1  He was subdued by passengers and crew when he ...        Mary Papenfuss  \n",
      "2  \"Until you have a dog you don't understand wha...         Elyse Wanshel  \n",
      "3  \"Accidentally put grown-up toothpaste on my to...      Caroline Bologna  \n",
      "4  Amy Cooper accused investment firm Franklin Te...        Nina Golgowski  \n"
     ]
    }
   ],
   "source": [
    "# Chuyển dữ liệu JSON thành DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "df = df.drop(columns=[\"link\", \"date\"])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         category  count_category\n",
      "0        POLITICS           35602\n",
      "1        WELLNESS           17945\n",
      "2   ENTERTAINMENT           17362\n",
      "3          TRAVEL            9900\n",
      "4  STYLE & BEAUTY            9814\n",
      "5       PARENTING            8791\n",
      "6  HEALTHY LIVING            6694\n",
      "7    QUEER VOICES            6347\n",
      "8    FOOD & DRINK            6340\n",
      "9        BUSINESS            5992\n"
     ]
    }
   ],
   "source": [
    "# Đếm category\n",
    "query = \"SELECT category, COUNT(category) AS count_category FROM df GROUP BY category ORDER BY count_category DESC LIMIT 10\"\n",
    "result = ps.sqldf(query, locals())\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Over 4 Million Americans Roll Up Sleeves For Omicron-Targeted COVID Boosters\n",
      "American Airlines Flyer Charged, Banned For Life After Punching Flight Attendant On Video\n"
     ]
    }
   ],
   "source": [
    "# Khai phá dữ liệu\n",
    "print(df['headline'].iloc[0])\n",
    "print(df['headline'].iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\duclh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Tiền xử lý dữ liệu bằng cách gộp headline, short_description, authors thành một chuỗi đầu vào\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "def preprocess_text(row):\n",
    "    text = row[\"headline\"] + \" \" + row[\"short_description\"] + \" \" + row[\"authors\"]\n",
    "    return text.lower().strip()\n",
    "\n",
    "df[\"text\"] = df.apply(preprocess_text, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels (category)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"category\"] = label_encoder.fit_transform(df[\"category\"])\n",
    "num_classes = len(label_encoder.classes_)  # Số lượng nhãn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize dữ liệu văn bản\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "MAX_LEN = 50  # Giới hạn độ dài câu\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = tokenizer(text, padding=\"max_length\", truncation=True, max_length=MAX_LEN, return_tensors=\"pt\")\n",
    "    return tokens[\"input_ids\"].squeeze(0), tokens[\"attention_mask\"].squeeze(0)\n",
    "\n",
    "df[\"tokens\"] = df[\"text\"].apply(lambda x: tokenize_text(x)[0])\n",
    "df[\"attention_masks\"] = df[\"text\"].apply(lambda x: tokenize_text(x)[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([167621, 50]), X_test: torch.Size([41906, 50])\n",
      "y_train: torch.Size([167621]), y_test: torch.Size([41906])\n",
      "masks_train: torch.Size([167621, 50]), masks_test: torch.Size([41906, 50])\n"
     ]
    }
   ],
   "source": [
    "# Chia dữ liệu thành tập huấn luyện và tập kiểm tra\n",
    "\n",
    "X = torch.stack(df[\"tokens\"].tolist())  # Dữ liệu đầu vào\n",
    "y = torch.tensor(df[\"category\"].values, dtype=torch.long)  # Nhãn\n",
    "masks = torch.stack(df[\"attention_masks\"].tolist())  # Attention masks\n",
    "\n",
    "# Chuyển tensor về numpy trước khi chia\n",
    "X_list = X.detach().cpu().tolist()\n",
    "y_list = y.detach().cpu().tolist()\n",
    "masks_list = masks.detach().cpu().tolist()\n",
    "\n",
    "# Chia tập dữ liệu (dùng random_state để tái lập kết quả)\n",
    "X_train_list, X_test_list, y_train_list, y_test_list, masks_train_list, masks_test_list = train_test_split(\n",
    "    X_list, y_list, masks_list, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Chuyển lại về tensor\n",
    "X_train, X_test = torch.tensor(X_train_list), torch.tensor(X_test_list)\n",
    "y_train, y_test = torch.tensor(y_train_list), torch.tensor(y_test_list)\n",
    "masks_train, masks_test = torch.tensor(masks_train_list), torch.tensor(masks_test_list)\n",
    "\n",
    "\n",
    "# In kích thước để kiểm tra\n",
    "print(f\"X_train: {X_train.shape}, X_test: {X_test.shape}\")\n",
    "print(f\"y_train: {y_train.shape}, y_test: {y_test.shape}\")\n",
    "print(f\"masks_train: {masks_train.shape}, masks_test: {masks_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Khởi tạo mô hình RNN/LSTM\n",
    "\n",
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "EMBED_DIM = 128  \n",
    "HIDDEN_DIM = 256  \n",
    "NUM_LAYERS = 2  \n",
    "DROPOUT = 0.5  \n",
    "LEARNING_RATE = 1e-3  \n",
    "EPOCHS = 10  \n",
    "BATCH_SIZE = 32  \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "embedding = nn.Embedding(VOCAB_SIZE, EMBED_DIM).to(device)\n",
    "lstm = nn.LSTM(EMBED_DIM, HIDDEN_DIM, num_layers=NUM_LAYERS, batch_first=True, dropout=DROPOUT).to(device)\n",
    "fc = nn.Linear(HIDDEN_DIM, num_classes).to(device)\n",
    "dropout = nn.Dropout(DROPOUT).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(list(embedding.parameters()) + list(lstm.parameters()) + list(fc.parameters()), lr=LEARNING_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chuyển dữ liệu thành DataLoader\n",
    "\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 2.3061\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m output \u001b[38;5;241m=\u001b[39m fc(dropout(final_hidden_state))\n\u001b[0;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, targets)\n\u001b[1;32m---> 17\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     20\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\duclh\\AppData\\Local\\anaconda3\\envs\\ml_venv\\lib\\site-packages\\torch\\_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    487\u001b[0m     )\n\u001b[1;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\duclh\\AppData\\Local\\anaconda3\\envs\\ml_venv\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Vòng lặp huấn luyện\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        inputs, targets = batch\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        embedded = embedding(inputs)\n",
    "        lstm_out, _ = lstm(embedded)\n",
    "        final_hidden_state = lstm_out[:, -1, :]\n",
    "        output = fc(dropout(final_hidden_state))\n",
    "        \n",
    "        loss = criterion(output, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {total_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 18\u001b[0m\n\u001b[0;32m     14\u001b[0m         output \u001b[38;5;241m=\u001b[39m fc(dropout(final_hidden_state))\n\u001b[0;32m     16\u001b[0m         _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(output, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m         y_pred\u001b[38;5;241m.\u001b[39mextend(\u001b[43mpredicted\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     19\u001b[0m         y_true\u001b[38;5;241m.\u001b[39mextend(targets\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m     21\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(y_true, y_pred)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "# Đánh giá mô hình\n",
    "\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        inputs, targets = batch\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        embedded = embedding(inputs)\n",
    "        lstm_out, _ = lstm(embedded)\n",
    "        final_hidden_state = lstm_out[:, -1, :]\n",
    "        output = fc(dropout(final_hidden_state))\n",
    "        \n",
    "        _, predicted = torch.max(output, 1)\n",
    "        \n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "        y_true.extend(targets.cpu().numpy())\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(classification_report(y_true, y_pred, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dự đoán với dữ liệu mới\n",
    "\n",
    "def predict_category(text):\n",
    "    tokenized_text, _ = tokenize_text(text)\n",
    "    tokenized_text = tokenized_text.unsqueeze(0).to(device)  # Thêm batch dimension\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embedded = embedding(tokenized_text)\n",
    "        lstm_out, _ = lstm(embedded)\n",
    "        final_hidden_state = lstm_out[:, -1, :]\n",
    "        output = fc(dropout(final_hidden_state))\n",
    "        _, predicted = torch.max(output, 1)\n",
    "\n",
    "    return label_encoder.inverse_transform([predicted.cpu().item()])[0]\n",
    "\n",
    "new_text = \"Apple announces new iPhone with AI-powered camera\"\n",
    "print(\"Predicted Category:\", predict_category(new_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lưu mô hình\n",
    "\n",
    "torch.save({\n",
    "    \"embedding\": embedding.state_dict(),\n",
    "    \"lstm\": lstm.state_dict(),\n",
    "    \"fc\": fc.state_dict()\n",
    "}, \"text_rnn_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
